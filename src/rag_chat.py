"""
RAG Chat v·ªõi LLM (Google Gemini)
S·ª≠ d·ª•ng Hybrid Search (BM25 + Semantic) ƒë·ªÉ retrieve context
"""

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from src.chunking.text_chunker import HybridSectionChunker
import os
from dotenv import load_dotenv

load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")


class RAGChat:
    
    def __init__(self, 
                 collection_name="knowledge_base",
                 persist_directory="data/chroma_db",
                 model_name="gemini-2.5-flash-lite",
                 temperature=0.1,
                 top_k=5,
                 bm25_weight=0.6,
                 semantic_weight=0.4):
        
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.top_k = top_k
        self.bm25_weight = bm25_weight
        self.semantic_weight = semantic_weight
        
        # Kh·ªüi t·∫°o chunker (d√πng ƒë·ªÉ retrieve)
        self.chunker = HybridSectionChunker(chunk_size=800, chunk_overlap=150)
        
        # Kh·ªüi t·∫°o LLM
        print(f"ü§ñ Kh·ªüi t·∫°o {model_name}...")
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            api_key=GEMINI_API_KEY,
            temperature=temperature,
            convert_system_message_to_human=True
        )
        
        # T·∫°o prompt template
        self.prompt = ChatPromptTemplate.from_messages([
            ("human", """B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh, tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n CONTEXT ƒë∆∞·ª£c cung c·∫•p.

NGUY√äN T·∫ÆC:
1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin trong CONTEXT
2. N·∫øu CONTEXT kh√¥ng c√≥ th√¥ng tin ‚Üí tr·∫£ l·ªùi "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu"
3. Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, CH√çNH X√ÅC, b·∫±ng ti·∫øng Vi·ªát
4. Tr√≠ch d·∫´n th√¥ng tin t·ª´ CONTEXT n·∫øu c√≥ th·ªÉ
5. KH√îNG b·ªãa ƒë·∫∑t ho·∫∑c suy ƒëo√°n th√¥ng tin kh√¥ng c√≥ trong CONTEXT

CONTEXT:
{context}

C√ÇU H·ªéI: {question}

TR·∫¢ L·ªúI:"""),
        ])
        
        # T·∫°o RAG chain
        self.rag_chain = (
            {
                "context": lambda x: self._format_docs(x["docs"]),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
        
        print(f"‚úÖ RAG Chat s·∫µn s√†ng!")
    
    def _format_docs(self, docs):
        if not docs:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."
        
        formatted = []
        for i, doc in enumerate(docs, 1):
            header = doc.metadata.get('h2', doc.metadata.get('h1', ''))
            formatted.append(f"--- ƒêo·∫°n {i} ({header}) ---\n{doc.page_content}")
        return "\n\n".join(formatted)
    
    def retrieve(self, query):
        print(f"\nüîç ƒêang retrieve context cho: '{query}'")
        
        try:
            results = self.chunker.query_with_hybrid_search(
                query=query,
                collection_name=self.collection_name,
                persist_directory=self.persist_directory,
                k=self.top_k,
                bm25_weight=self.bm25_weight,
                semantic_weight=self.semantic_weight
            )
            
            print(f"üì¶ ƒê√£ retrieve {len(results)} chunks")
            return results
        except FileNotFoundError as e:
            print(f"‚ùå L·ªói: {str(e)}")
            raise FileNotFoundError(
                f"Vector database ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. "
                f"Vui l√≤ng ch·∫°y script kh·ªüi t·∫°o database tr∆∞·ªõc. "
                f"Chi ti·∫øt: {str(e)}"
            )
        except Exception as e:
            print(f"‚ùå L·ªói khi retrieve: {str(e)}")
            raise RuntimeError(f"L·ªói khi retrieve context: {str(e)}")
    
    def chat(self, question, verbose=False):
        
        # Validate question
        if not question or not question.strip():
            return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi h·ª£p l·ªá."
        
        try:
            # Retrieve context
            docs = self.retrieve(question)
            
            # Ki·ªÉm tra c√≥ docs kh√¥ng
            if not docs:
                return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."
            
            # In context n·∫øu verbose
            if verbose:
                print(f"\n{'='*70}")
                print(f"CONTEXT ƒê∆Ø·ª¢C RETRIEVE:")
                print(f"{'='*70}")
                for i, doc in enumerate(docs, 1):
                    print(f"\nüìÑ Chunk {i}:")
                    print(f"   Headers: {doc.metadata.get('h1', '')} / {doc.metadata.get('h2', '')}")
                    print(f"   Content: {doc.page_content[:200]}...")
                    print(f"   {'-'*70}")
            
            # Format context
            context = self._format_docs(docs)
            
            # ƒê·∫£m b·∫£o context kh√¥ng r·ªóng
            if not context or not context.strip():
                return "Xin l·ªói, kh√¥ng th·ªÉ t·∫°o context t·ª´ t√†i li·ªáu."
            
            # Generate answer
            print(f"\nüí¨ ƒêang generate c√¢u tr·∫£ l·ªùi...")
            answer = self.rag_chain.invoke({
                "docs": docs,
                "question": question
            })
            
            return answer
            
        except FileNotFoundError as e:
            error_msg = (
                "Vector database ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. "
                "Vui l√≤ng ch·∫°y script kh·ªüi t·∫°o database tr∆∞·ªõc khi s·ª≠ d·ª•ng chatbot."
            )
            print(f"‚ùå {error_msg}")
            return error_msg
            
        except Exception as e:
            error_msg = f"ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"
            print(f"‚ùå {error_msg}")
            return error_msg



# ============================================================================
# INTERACTIVE CHAT
# ============================================================================

def interactive_chat():
    rag = RAGChat(
        collection_name="knowledge_base",
        persist_directory="data/chroma_db",
        model_name="gemini-2.5-flash-lite",
        temperature=0.1,
        top_k=10,
        bm25_weight=0.5,
        semantic_weight=0.5
    )
    
    print(f"\n{'='*70}")
    print(f"ü§ñ RAG CHAT - H·ªèi ƒë√°p v·ªÅ Doanh nh√¢n")
    print(f"{'='*70}")
    print(f"Nh·∫≠p 'quit' ho·∫∑c 'exit' ƒë·ªÉ tho√°t")
    print(f"Nh·∫≠p 'verbose' ƒë·ªÉ b·∫≠t/t·∫Øt hi·ªÉn th·ªã context")
    print(f"{'='*70}\n")
    
    verbose = False
    
    while True:
        question = input("‚ùì C√¢u h·ªèi: ").strip()
        
        if question.lower() in ['quit', 'exit', 'tho√°t']:
            print("üëã T·∫°m bi·ªát!")
            break
        
        if question.lower() == 'verbose':
            verbose = not verbose
            print(f"‚úÖ Verbose mode: {'ON' if verbose else 'OFF'}")
            continue
        
        if not question:
            continue
        
        try:
            answer = rag.chat(question, verbose=verbose)
            print(f"\nüí° TR·∫¢ L·ªúI: {answer}\n")
        except Exception as e:
            print(f"‚ùå L·ªói: {e}\n")


if __name__ == "__main__":
    interactive_chat()

from src.chunking.text_chunker import HybridSectionChunker
from src.rag_chat import interactive_chat
import os
import pickle
import traceback

from langchain_community.vectorstores import Chroma
from src.ingestion.get_data_from_wikipedia import get_html_page_from_wikipedia
from src.preprocessing.html_cleaner import clean_wikipedia_html
from src.preprocessing.normalize_markdown import convert_html_to_normalized_md

def print_banner():
    print("\n" + "="*80)
    print(" "*25 + "ğŸš€ RAG SYSTEM - Há»’ CHÃ MINH KB")
    print("="*80)


def print_menu():
    print(f"\n{'='*80}")
    print("CHá»ŒN CHá»¨C NÄ‚NG:")
    print(f"{'='*80}")
    print("1. ğŸ“ Chuáº©n bá»‹ data")
    print("2. ğŸ”ª Chunking vÃ  lÆ°u vÃ o Vector DB")
    print("3. ğŸ’¬ RAG Chat (Interactive)")
    print("4. ğŸš€ Cháº¡y cáº£ hai (Chunking â†’ Chat)")
    print("0. âŒ ThoÃ¡t")
    print(f"{'='*80}")

def prepare_data():
    search_keyword = input("HÃ£y nháº­p keyword tÃ¬m kiáº¿m trÃªn Wikipedia (máº·c Ä‘á»‹nh 'Há»“ ChÃ­ Minh') vÃ  nháº¥n Enter: ").strip()
    if not search_keyword:
        search_keyword = "Há»“ ChÃ­ Minh"

    # Láº¥y cÃ¡c file html tá»« wikipedia
    html_file_path = get_html_page_from_wikipedia(search_keyword)

    # LÃ m sáº¡ch html
    cleaned_html_file_path = clean_wikipedia_html(html_file_path)

    # Chuyá»ƒn html thÃ nh markdown vÃ  chuáº©n hÃ³a markdown
    convert_html_to_normalized_md(cleaned_html_file_path)

# HÃ m chunking cÃ¡c file markdown vÃ  lÆ°u vÃ o Chroma DB
def chunking():
    print("\n" + "="*80)
    print("ğŸ”ª CHUNKING Táº¤T Cáº¢ FILE MARKDOWN VÃ€ LÆ¯U VÃ€O VECTOR DB")
    print("="*80)
    
    md_dir = "data/processed_data"
    
    if not os.path.exists(md_dir):
        print(f"\nâŒ ThÆ° má»¥c khÃ´ng tá»“n táº¡i: {md_dir}")
        return None
    
    # Láº¥y táº¥t cáº£ file .md
    md_files = [f for f in os.listdir(md_dir) if f.endswith('.md')]
    
    if not md_files:
        print(f"\nâŒ KhÃ´ng tÃ¬m tháº¥y file markdown nÃ o trong {md_dir}")
        return None
    
    print(f"\nğŸ“ TÃ¬m tháº¥y {len(md_files)} file markdown:")
    for i, f in enumerate(md_files, 1):
        print(f"  {i}. {f}")
    
    # collection_name = input("\nNháº­p tÃªn collection (hoáº·c Enter Ä‘á»ƒ dÃ¹ng 'knowledge_base'): ").strip()
    # if not collection_name:
    #     collection_name = "knowledge_base"
    collection_name = "knowledge_base"
    
    try:
        chunker = HybridSectionChunker(chunk_size=800, chunk_overlap=150)
        all_chunks = []
        
        print(f"\nğŸ”„ Báº¯t Ä‘áº§u chunking...")
        
        for idx, md_file in enumerate(md_files, 1):
            md_file_path = os.path.join(md_dir, md_file)
            file_name = md_file.replace('.md', '')
            
            print(f"\nğŸ“„ [{idx}/{len(md_files)}] Äang xá»­ lÃ½: {md_file}")
            
            # Load document
            with open(md_file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # TÃ¡ch theo headers
            section_docs = chunker.section_splitter.split_text(text)
            print(f"   â†’ {len(section_docs)} sections")
            
            # Recursive split cho sections lá»›n
            file_chunks = []
            for section_idx, section_doc in enumerate(section_docs):
                # ThÃªm metadata: source file name
                section_doc.metadata.update({
                    "source": md_file_path,
                    "section_id": section_idx,
                    "document": file_name
                })
                
                # Náº¿u section quÃ¡ lá»›n, tÃ¡ch tiáº¿p
                if len(section_doc.page_content) > chunker.chunk_size:
                    sub_chunks = chunker.recursive_splitter.split_documents([section_doc])
                    
                    # ThÃªm sub_chunk_id vÃ  giá»¯ source_file metadata
                    for sub_idx, sub_chunk in enumerate(sub_chunks):
                        sub_chunk.metadata.update({
                            "sub_chunk_id": sub_idx,
                            "total_sub_chunks": len(sub_chunks),
                        })
                    file_chunks.extend(sub_chunks)
                else:
                    file_chunks.append(section_doc)
            
            print(f"   â†’ {len(file_chunks)} chunks")
            all_chunks.extend(file_chunks)
        
        print(f"\nğŸ“Š Tá»”NG Káº¾T:")
        print(f"   - Tá»•ng sá»‘ file: {len(md_files)}")
        print(f"   - Tá»•ng sá»‘ chunks: {len(all_chunks)}")
        
        # LÆ°u vÃ o Chroma
        print(f"\nğŸ’¾ Äang lÆ°u vÃ o Chroma DB...")
        
        persist_directory = "data/chroma_db"
        
        # XÃ³a DB cÅ© náº¿u cÃ³
        if os.path.exists(persist_directory):
            import shutil
            print(f"ğŸ—‘ï¸  XÃ³a DB cÅ©...")
            shutil.rmtree(persist_directory)
        
        os.makedirs(persist_directory, exist_ok=True)
        
        Chroma.from_documents(
            documents=all_chunks,
            embedding=chunker.embeddings,
            collection_name=collection_name,
            persist_directory=persist_directory
        )
        
        # LÆ°u chunks vÃ o pickle
        chunks_file_path = os.path.join(persist_directory, f"{collection_name}_chunks.pkl")
        with open(chunks_file_path, 'wb') as f:
            pickle.dump(all_chunks, f)
        print(f"ğŸ’¾ ÄÃ£ lÆ°u chunks vÃ o {chunks_file_path}")
        
        print(f"\nâœ… ÄÃƒ HOÃ€N THÃ€NH!")
        print(f"   ğŸ“¦ Collection: {collection_name}")
        print(f"   ğŸ’¾ LÆ°u táº¡i: {persist_directory}")
        
        return collection_name
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        traceback.print_exc()
        return None


def rag_chat():
    """Giai Ä‘oáº¡n 4: RAG Chat"""
    print("\n" + "="*80)
    print("ğŸ’¬ GIAI ÄOáº N 4: RAG CHAT")
    print("="*80)
    
    try:
        interactive_chat()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ÄÃ£ thoÃ¡t chat!")
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")


def run_full_pipeline():
    print("\n" + "="*80)
    print("ğŸš€ CHáº Y TOÃ€N Bá»˜ PIPELINE")
    print("="*80)
    
    # Chuáº©n bá»‹ data (wiki => html => cleaned html => normalized md)
    prepare_data()

    # Chunking vÃ  lÆ°u vÃ o vector DB
    chunking()
    
    # Báº­t rag chat trong terminal
    input("\nâœ… Pipeline hoÃ n táº¥t! Nháº¥n Enter Ä‘á»ƒ vÃ o RAG Chat...")
    rag_chat()

def main():
    print_banner()
    
    while True:
        print_menu()
        choice = input("\nğŸ‘‰ Chá»n giai Ä‘oáº¡n (0-4): ").strip()
        
        if choice == '0':
            print("\nğŸ‘‹ Táº¡m biá»‡t!")
            break
        elif choice == '1':
            prepare_data()
        elif choice == '2':
            chunking()
        elif choice == '3':
            rag_chat()
        elif choice == '4':
            run_full_pipeline()
        else:
            print("\nâŒ Lá»±a chá»n khÃ´ng há»£p lá»‡!")
        
        if choice != '0':
            input("\nâ¸ï¸  Nháº¥n Enter Ä‘á»ƒ tiáº¿p tá»¥c...")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Táº¡m biá»‡t!")
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
